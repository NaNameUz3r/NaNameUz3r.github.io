---
title: "Сложность и эффективность алгоритмов."
tags: "Программирование Алгоритмы ComputerScience"
lang: ru
show_edit_on_github: false
comment: true
license: false
modify_date: "2022-02-06"
show_subscribe: false
article_header:
  theme: dark
  type: overlay
  align: left
  background_image:
    src: /header_images/asymptotics.jpg
---

Как правильно думать о программе, что такое временная сложность и это самое "О большое". Давайте разбираться.

<!--more-->

Недавно я сделал перевод важной статьи — [Алгоритм прохождения интервью по программированию](/2022/01/15/programming_interview_algorithm.html). Это сложная тема, особенно если вы никогда до этого не изучали структуры данных, ничего не слышали про временную сложность и не занимались оценкой эффективности или не эффективности программного кода. 

## А как думать?

Начну с того, что не зависимо от особенностей каждого человека, если мы физически и психически здоровы то "понимание" какой-то темы у нас складывается примерно одинково. Возьмем, например, телевизор. Человек растет и развивается, видит как его родители нажимают на кнопки, из телевизора показывают картинки, в его мозгу формируются некоторые связи приводящие к пониманию зависимости действий над объектом, и к каким результатам эти действия приводят. Но если человек никогда не интересовался как работает телевизор, из каких компонентов он состоит, то он не сможет объяснить что это на самом деле такое, как работает, как это создать с нуля или починить. Это нормально. Инженер или работник завода на котором собирают телевизоры или его компоненты знают порядком больше, в виду обладания *большим опытом* в непосредственной области, соответствующим обучением, etc. Надо ли говорить как телевизор будет воспринимать пещерный человек, или коренной житель глубокой тропической деревни, где иногда до сих пор практикуют каннибализм?

С программированием всё обстоит так-же. Информационные технологии и программирование — дети науки и математики, развивающиеся лучшими умами человечества последние столетия.
Само собой это сложная тема, и нахрапом её не взять.

Когда мы программисты-самоучки, мы можем достаточно быстро выучить современный высокоуровневый и выразительный язык программирования, например Python, методы его стандартной библиотеки. Мы интуитивно начинаем собирать в голове образ того что мы хотим добиться от программы, смутно представляем как она работает. Мы, ещё раз, *интуитивно* понимаем что такое переменная, что такое лист или массив. Ну есть там какие-то данные, мы их туда записали, можем сделать клац-клац **sorted(my_very_spetial_list)** и знаем чего ожидать. Несмотря на то что таким образом можно изучить ряд технологий, фреймворков, и довольно успешно выполнять простые прикладные задачи, это представление **неверно**, контрпродуктивно и ничем хорошим не заканчивается, например мы это очень хорошо прочувствуем как только попытаемся разработать хоть сколько нибудь сложную систему.

Поэтому единственный правильный способ думать о программе — это думать научно, и так же подходить к изучению программирования. Напрягаем извилины и идём дальше. 

## Алгоритм

#### Идем от интуитивного к научному.

Что такое программирование вообще? Ну, в первую очередь, скажете вы — это написание какой-то программы (текста), которая что-то внутри делает и выдает интересующий нас результат, и будете в общем-то правы. Приемлемо. А что такое программа? Ни больше ни меньше, программа, в самом простом её представлении и правильном понимании — это *Алгоритм* достижения требуемого результата. Утрированный пример алгоритма, который я неоднократно встречал во всевозможных книжках начального уровня, это список ваших последовательных дел на завтра:
- Проснуться
- Потянуться
- Надеть носки
- Поехать на работу (или дойти до компьютра и начать рабоать :) )
- Прерваться на обед

При том каждый из пунктов тоже на самом деле является алгоритмом некоторых последовательных (а может и параллельных) действий, так что в примере выше я скоре описываю *список алгоритмов*.
Так и есть. В любой даже маленькой программе практически всегда есть *n* число алгоритмов — вызовы разных функций, каждая из которых что-то вычисляет.
А если вычисляет — значит алгоритм. Кажется я вас начинаю запутывать, но думаю идея ясна. 

*Алгоритмом мы называем пошаговое руководство "компьютеру" (исполнителю) что-то нам посчитать. Вычисления могут быть очень простыми, а могут быть и крайне сложными. Правильный способ думать о программе не как о волшебном коде, а как о вполне строгом, **математическом** описании решения задачи.*

Вычислительное железо, как и любой другой объект физического мира подчиняется законам физики. Сила притяжения (по крайней мере на нашей планете) не даст вам прыгнуть кузнечиком в окно второго этажа. Вычислительные ресурсы (центральный процессор, оперативная память, etc) просто так не обработают по щелчку пальцев огромное количество данных — на это потребуется **время**. 

К какому бы крутому суперкомпьютеру у вас вдруг не оказался доступ, его мощность играет *намного меньшее значение* для производительности программы, чем *правильно подобранный алгоритм*. Именно о правильном подборе алгоритма и идет речь в переведенной мною статье, со ссылки на которую начинается этот пост.

Итак, алгоритм — это вычисление или преобразование *входных* данных в интересующие нас *выходные* данные. Вы любите ждать? Маловероятно. Проще всего эффективность алгоритма понять как отношение времени, за которое мы получим результат. Чем быстрее — тем лучше. 

## Асимптотическая сложность

Так что же влияет на скорость работы алгоритма? Как сказанно выше, в первую очередь это правильность самого алгоритма. А точнее — оправданность его использования в *конкретных обстоятельствах*. Под конкретным обстоятельствами я имею в виду контекст решаемой нами задачи. В контексте задачи скорость, с которой алгоритм будет вычислять решение, зависит от *объема (количества) входных данных*. 

Научно (математически) сложность алгоритмов оценивается с помощью так называемого *асимптотического анализа*. Мы рассматриваем сложность алгоритма посредством анализа *зависимости времени и вычислительных ресурсов*, которые потребуются алгоритму в процессе его работы поставленной задачей.

На практике всё это выражается как математическая функция, которая позволяет понять *насколько быстро* будет увеличиваться время работы алгоритма, в зависимости от увеличения обрабатываемого объема данных. 

Иными словами, под *асимптотикой* мы понимаем *характер* с которым изменяется функция в течении *времени* (хуже или лучше она начинает работать), когда её аргумент (количество обрабатываемых данных) *стремится* к какой-то точке. 

Только в асимптотическом анализе сложности *компьютерых алгоритмов* за эту точку принимается бесконечность, потому что нас интересует не фактическое время выполнения конкретного алгоритма (секунды,etc), а то как будет меняться эффективность работы, если мы будем постоянно и бесконечно повышать размер входных данных.

Простите меня за то что я несколько раз повторяю одно и то же, но это важно понять:

*Мы оцениваем скорость работы алгоритма в сравнительном **количестве операций***. Этим количеством мы и меряем "время".

---

### Так что там за О?

Наиболее часто для оценки роста используются следующие [нотации](https://en.wikipedia.org/wiki/Mathematical_notation):
- Ο (О-большое) – верхняя или "наихудшая" оценка сложности. Входные данные подаются самым "неудобным" для алгоритма способом;
- o (o-малое) – средняя оценка, данные подаются случайным образом;
- Ω (Омега) – нижняя оценка, идеальная ситуация для достижения максимальной скорости алгоритма;
- Θ (Тета) – нижняя и верхняя, точная оценка роста временной функции.

Пока мы рассмотрим фундаментальные меры сложности, использумые повсеместно, которые выражаются через O-большое.

#### Разбираемся подробнее

Возьмем *n* за величину подаваемого объема данных. Тогда верхнюю оценку сложности алгоритма можно записать как **O(f(n))**. Почему нас интересует именно верхняя или "наихудшая" оценка? Потому что с практической точки зрения нам важно понять этот *наихудший вариант развития событий* при котором алгоритму будет скормлено *большое количество данных*. Если он будет эффективен в этих условиях, то разумеется будет эффективен и в более "благоприятных".

Самые простые для понимания обозначения сложности это **O(1)** и **O(n)**.

**O(1)** выражает сложность, при которой для выполнения алгоритму потребуется *константа* времени. Например взятие элемента из массива по его индексу. Мы точно знаем где элемент находится и просто идём туда. *Такие операции не зависят от количества обрабатываемых данных.*

**O(n)** линейно зависит от количества входных данных. В худшем случае такому алгоритму придется проводить какие-то операции с каждым элементом данных. Это, например, обход массива, обход связанного списка или расчет его длинны. Чтобы понять лучше приведу утрированный пример. Представим что задача состоит в том, чтобы определенному сущетву (алгоритм) нужно выпить некоторое (n) количество воды. Смоделируем ситуацию, в которой воду пьёт не человек, а волшебный троглодит, у которого не желудок, а бездонная черная дыра (n стремится к бесконечности). Пить воду он может только последовательно (ограничения реализации), глоток за глотком, поэтому алгоритм выпивания воды будет иметь линейную сложность O(n) — чем больше воды мы дадим троглодиту на испитие, тем дольше он будет её хлебать.

---

Классическим вариантом логарифмической сложности **O(log n)**, где за основание логарифма обычно берется 2, является бинарный (двоичный) поиск. Чем больше входных данных — тем медленнее растет время выполнения алгоритма (тем меньше его сложность). Самый простой и затертый до дыр пример для описания работы двоичного поиска — попытка найти номер телефона нужного вам человека по индексированному справочнику (Аббоненты упорядочены по алфавиту). Вы можете искать по порядку, запись за записью, страница за страницей. Очевидно что это может занять много времени. Сложность такого глупого поиска можно выразить предыдущей формулой O(n). Представьте что вы в ищите телефон человека не в своей записной книжке, а в справочнике всех аббоненов Москвы. Похоже что листать его последовательно (линейно) плохая идея. 

Намного быстрее, зная что справочник упорядочен, открыть его примерно в том месте, где мы ожидаем найти нужного аббонента. Например мы ищем Фёдора Корнельчука (все совпадения случайны!). Зная алфавит русского языка может показаться что буква К обязательно будет находиться в первой половине справочника, но это не точно. Мы не знаем сколько фамилий на А, Б, и так далее. Может случиться так, что фамилии на К находится во второй половине сравочника. Алгоритм Двоичный поиска работает так — мы открываем справочник (массив) посередине и смотрим куда попали, если нам нужно в левую половину (на откртытой странице фамилии, например, на букву П), мы отбрасываем правую половину и открываем оставшуюся левую *снова посередине*. И так до тех пор пока не найдем Корнельчука.

---

Сложность алгоритмов сортировки часто выражается как **O(n log n)**. Время выполнения таких алгоритмов растет быстрее, чем O(n). Можно представить как комбинацию O(log n) и O(n).

Далее по интенсивности роста идут квадратичные зависимости, например **O(n<sup>2</sup>)**. Время выполнения таких алгоритмов растет очень быстро при увеличении объема входных данных (экспоненциально!). Например, два цикла, когда один вложен в другой, каждый из которых работает за O(n). Практически в любом случае это **очень плохая мера**, которую вообще не стоит определять в своих алгоритмах. 
 
---

Разбор алгоритмов сортировки и разных структур данных выходит за рамки этого поста. В будущем мы разберем базовые структуры данных и попытаемся оценить их сложность. Надеюсь что теперь стало понятнее о чем шла речь в статье-алгоритме Майкла.

До новых встреч!









