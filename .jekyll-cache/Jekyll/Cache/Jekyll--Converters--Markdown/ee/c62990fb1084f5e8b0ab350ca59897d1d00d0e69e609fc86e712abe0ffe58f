I"	w<p>Научный взгляд на программирование. Concurrency, atomicity и кое что ещё.
<!--more--></p>

<p>Сейчас я прохожу курс Бобровского Сергея Игоревича, который называется “Как понять в программировании всё”. На самом деле это цикл курсов, направленный на освоение научного, или лучше сказать — инженерного взгляда на программироваание, от фундаментальных концепций до осмысления <em>парадигм</em> программирования.</p>

<p>Этот пост является своего рода отчетом о пройденном материале. Хочу сфокусироваться на теме параллельного программирования и атомарности — я застрял на тесте по этой теме :), надеюсь вместе мы разберемся. В дополнение я постараюсь освятить некоторые важные концепции. Остальная часть курса, которая в этом после не затрагивается, либо связана с синтаксисом и особенностями мультипарадигмального языка программирования Julia, либо это отдельные и сложные концепции, которые мы затроули пока лишь поверхностно, но обязательно вернемся к их углубленному изучению позже (или я их плохо понял). Lets rock.</p>

<h3 id="что-там-научного-то">Что там научного-то?</h3>

<p>В посте про <a href="/2022/01/22/algorithms_complexity.html">сложность алгоритмов</a> я пытался развить мысль о том, что программирование — дитя математики, и подходить к анализу и понимаю алгоритмов нужно с научно-математической позиции мышления, а не с интуитивно-наивной. С программированием всё обстоит точно-так же. Чтобы разрабатывать эффективные и надежные программные системы необходимо понимать как работает “под капотом” сама система, и инструменты которые для разработки этой системы используются (языки программирования, парадигмы или подходы к программированию, etc.).</p>

<h3 id="фундаментальные-концепции">Фундаментальные концепции</h3>

<h4 id="переменная">Переменная</h4>

<p>Программе, или алгоритму, в процессе работы необходимо “пространство” в котором будут “производиться вычисления”. Этим пространством является оперативная память и процессорные кеши. Их устройство это отдельная и сложная тема, сейчас нам нужно понимать что есть некоторый <em>ресурс</em> в который программа может быстро записывать какие-то данные, и читать их откуда.</p>

<p>Проще всего представить оперативную память как набор ячеек у каждой из которых есть адрес. По этому адресу к ячейками можно <em>получить доступ</em>. В данном случае имеются в виду <strong>физические</strong> ячейки оперативной памяти.</p>

<p>В коде программы, в частном случае, мы напрямую (явно) с этими адресами не работаем, а используем <em>переменные</em>,к которым привязываем <em>значения</em>. Имя переменной — это её идентификатор, а значениями являются какие-то данные, которые мы этому этому имени <em>присваиваем</em>, чаще всего с помощью оператора “=”.</p>

<p>Ещё более научно это называется <em>именованным состоянием</em>, что уже чуть более сложная концепция, которая является одной из главных характеристик в <em>любой</em> парадигме программирования, а её реализация (степень поддержки) зависит от конкретно рассматриваемой парадигмы.</p>

<p>В общем и целом, <em>состояние</em> — это “способность” программы запоминать данные (присвоенные значения),и в будущем с ними как-то работать. Если состояние имеет идентификатор, то есть, напрямую и явно доступно для операций в ходе программирования, то оно называется <em>“именованным состояним”</em>. Да, состояния могут не иметь идентификаторов и быть своего рода <em>безымянными</em>.</p>

<h5 id="зачем-идентификаторы-есть-у-ячеек-есть-адреса">Зачем идентификаторы, есть у ячеек есть адреса?</h5>

<p>Во-первых, как я уже писал, мы с адресами напрямую не работаем. Во-вторых, на самом деле значения ячейкам памяти присваиваются как “константа”, то есть “на постоянку”, или однократно.</p>

<p><em>“Однократное присваивание”</em> это одна из фундаментальных концепций программирования, суть которой заключается в том, что как только в коде определяется новая переменная (некоторому идентификатору присваивется его “первое” значение), эта переменная становится неизменяемой (иммутабельной).</p>

<p>Подождите, но я могу сделать в своем любимом пайтоне что-то вроде: my_special_var = “hello dear friend”, и следом my_special_var = “goodbye fella”, и расшибусь головой о стену от уверенности в том что print(my_special_var) вернёт последнюю строку — “goodbye fella”. Всё так! Просто почти во всех современных языках программирования кроме концепции однократного присваивания переменных существует дополнительный механизм — “явное определение”, которое позволяет присвоить <em>новые</em> значения уже <em>существующему</em> идентификатору. Но для записи данных использоваться, как правило, будет <strong>не та же самая ячейка памяти</strong>, а новая и с другим адресом. <em>Тот же самый, уже существующий идентификатор (имя переменной)</em> будет связан с другой областью памяти, в которую будет записано новое значение.</p>

<p>Не увлекайтесь “явным <em>переопределением</em>”, это удобно, но есть один <strong>серьезный минус</strong> — потенциально эта возможность легкого переопределения приводит к куче ошибок. Относитесь к переменным чутко и ответствено, используйте адекватный нейминг. Старайтесь не переопределять уже существующие переменные (если нужно, используйте новые имена соответствующие контексту изменений и “смыслу кода”). Но так же не стоит плодить явно лишние переменные, а необходимые определяйте в непосредственной близости к коду, в котором эти переменные используются. Это короткое отступление о базовых приницпах “ясного стиля” программирования :)</p>

<h3 id="опять-математика">Опять математика?</h3>

<p>Как было сказано ранее, реализация “способности запоминать состояния” определяется поддерживаемыми языком программирования парадигмами программирования.</p>

<p style="text-align: center;"><img src="/images/mind_blowing.gif" alt="Взрыв Мозга" /></p>
<p><em><center>Поддерживаемые языком программирования парадигмы программирования — Чего-чего? </center></em></p>

<p>Не волнуйтесь, мы разберемся.</p>

<h4 id="парадигма-программирования">Парадигма программирования</h4>

<p>Парадигмой программирования, грубо говоря, называется определённый подход к программной разработке. Правильно говоря — это не менее чем <strong>настоящее</strong> научное открытие, строго выверенная математическая теории, которая на практике воплащается в таких сложных программных системах как <em>языки программированя</em>. Это не значит что язык программирования конструируется на базе одной парадигмы, напротив — многие современные языки являются мультипарадигмальными, из чего следует вывод что это <em>ну очень сложные математические модели</em>.</p>

<p><em>Хорошая новость в том, что если что-то придумали одни люди, мы можем в этом разобраться, пусть даже приложив N усилия!</em></p>

<p>Так вот, эта математическая модель выражается как <em>семантика языка программирования</em>, которая передает смысл конструкций (и инструкций) в конкретном языке.</p>

<p>Подробное изучение парадигм программирования явно выходит за рамки этого поста и моих текущих знаний, но в будущем эти темы будут подробнее изучаться на соответсвующих курсах, до которых я надеюсь добраться, а значит о них будут написаны подобные посты-отчеты.</p>

<hr />

<p>Способ реализации <em>именованного состояния</em> является одной из двух ключевых характеристик парадигм программирования. Вторая характеристика — характерность или не характерность <strong>недетерминизма</strong>.</p>

<h4 id="недетерминизм">Недетерминизм</h4>

<p>В привычном представлении новичка-программиста, особенно самоучки, обычная императивная программа выполняется последовательно и выдает от вызова к вызову, с одними и теми-же аргументами, один и тот-же результат. Такое поведение назвыается <strong>детерминированным</strong>. Иными словами, если программа детерминирована, мы можем по её коду однозначно определить какой результат будет получен на выходе. Никаких сюрпризов.</p>

<p>Недетерминизм, как несложно предположить, является противоположной ситуацией, когда вызывая программу, даже с одними и теми же входными данными, мы можем наблюдать разный результат из раза в раз. Такой недетерминизм называется <em>явным</em>.</p>

<p>Но как такое получается? Программа может выдавать неожиданные результаты, если она описана в системе (парадигме) программирования, в которой соединяются уже изученные нами “именованные состояния” и <strong><em>параллельные</em></strong> вычисления.</p>

<h3 id="параллелизм">Параллелизм</h3>

<p>Наконец мы подобрались к параллелизму! Сперва нам нужно строго определять о чем идет речь. В русском языке часто используется одно и то же слово для фактически разных концепции.</p>

<p>Во-первых, есть parallelism — аппаратная концепция. Это про “одновременное” выполнение процессорных инструкций на нескольких ядрах, etc.</p>

<p>Во-вторых, concurrency — чисто программная концепция, которая нас интересует в первую очередь в рамках этой темы.</p>

<hr />

<p>Вернемся к связи недетерминизма с параллельными вычислениями и именованными состояниями. В теме программного параллелизма есть две разные концепции:</p>
<ul>
  <li>Процесс</li>
  <li>и Thread (нить).</li>
</ul>

<p>Вспоминая свое начальное обучение на администратора баз данных, одно из самых ярких впечатлений на тот момент у меня вызвало простое, но чертовски понятное определение разницы этих двух терминов, которое я услышал в одой из лекций Дмитрия Кетова на тему устройства Линукса. Дословно не помню, но суть такая:</p>

<p>“Процесс — вычисления работающие в <strong>изолированной</strong> модели памяти. Нити — работают в модели <strong>общей</strong> памяти.”</p>

<p>Иными словами, Нити — это параллельные “процессы” которые могу иметь\имеют доступ к одним и тем же <em>именованным состояниям</em> (модель “общей” памяти).</p>

<p>Теперь должно стать понятно откуда могут взяться “неожиданные”, недетерминированные результаты работы кода. Если в программе используются параллельные потоки (нити), которые работают с одними и теми-же переменными, мы <strong>вообще</strong> не можем знать в каком порядке эти нити буду запущены, изменят значения переменной и завершатся. В таких условиях работы возникает <em>race condition</em>, по русски — “состояние гонки”. Смысл этого термина в том, что “победителем” гонки будет нить, которая последней запишет в переменную результат своей работы. Отсюда и возникает <em>явный недетерминизим</em>, мы видим его в непосредственном результате работы программы.</p>

<p>Хотя существуют ситуации, в которых недетерминизм приемлем или даже необходим (является целевым, ожидаемым поведением), чаще всего мы хотим чтобы наши программы были детерминированы (надежны и стабильны), даже если мы применяем параллельные вычисления, например, используя их для ускорения работы программы.</p>

<p>Такой результат может быть достигнут только благодаря правильному проектированию программного кода, <strong>нельзя</strong> бездумно использовать параллельные вычисления и именованные состояния вместе, это приведет уже не просто к недетерминированному результату, а к настощему хаосу.</p>

<p>В случае когда нам всё таки нужно совмещать эти концепции, <strong>мы должны</strong> делать это в строго ограниченной, изолированной части системы, и код этот должен быть “грамотным” и выразительным.</p>

<p>Разумеется, в программной инженерии разработаны подходы к реализации совмещения переменных и concurrency.</p>

<h3 id="атомарность">Атомарность</h3>

<p>Один из способов решить проблему race condition — использовать атомарных операций.</p>

<p>Сначала вернемся к “состоянию гонки”, и разберем его чуть подробнее. Представьте толкучку. Именно это и происходит когда параллельные нити пытаются работать с одними и теми же участками памяти. Кто успел — тот и съел. Потоки вычислений пытаются выполнить похожие действия и мешают друг другу.</p>

<p>Способ борьбы с этой ситуаций — построение thread-safe, или органазиация “<em>безопасности потока</em>”. В язык программирования внедряются специальные инструкции, доступные разработчику для использования, которые позволяют объявить некоторые операции <em>атомарными</em>.</p>

<p>На практике это выглядит как <em>блокирование доступа к общей ячейке памяти</em> для других нитей, во время работы <em>“активной”</em> нити (та, которая “повесила” блокировку). Блокировка снимается когда активная нить завершила все необходимые ей операции с заблокированной ячейкой памяти.</p>

<p>Важный момент заключается в том, что хотя благодаря атомарности мы и изолируем ресурсы на время работы одной нити от других, сами они всё ещё могут, и запускаются в <strong>непредсказуемом</strong> порядке. То есть получается, что хотя thread-safe и является мощным инструментом-концепцией для устранения race condition, он не <em>избавляет нас от недеретминизма</em>.</p>

<p>Избавиться от недеретминизма при параллельных вычислениях можно, опять же, только правильным проектированием программы. Мы можем разделить атомарные операции по нитям таким образом, когда они, пусть и работают параллельно — результат становится детерминированным.</p>

<h3 id="железная-параллельность">Железная параллельность</h3>

<p>Хочется вкратце рассмотреть параллельность на уровне железа, работу многороцессорных систем.</p>

<p>Нам нужно понимать, что сначала именованные состояния попадают в ячейки оперативной памяти, и следом копируются по системной шине в <em>процессорные кэши</em>. Так как процессорных ядер у нас может быть несколько, параллельно работающие инструкции на разных ядрах могут стягивать в свой <em>локальный</em> кэш значения из памяти и по своему обрабатывать. В результате может получиться такая ситуация, когда в оперативной памяти у нас “изначальное” значение, а в локальных процессорных кэшах свои копии данных, да ещё и разные.</p>

<p>Разумеется, на практике существует решений этой проблемы, иначе как бы эта параллельность вообще работала? Мы точно не хотим иметь такой “race condition” на уровне железа, когда в операвную память будут возвращаться разные результаты работы из независимых (параллельно работающих) ядер.</p>

<p>Единство значений в локальных процессорных кэшах, для обеспечения корретной работы системы в целом, достигается благодаря так называемым <em>протоколам когерентности</em>.</p>

<p>Когерентность кэша (cache coherence) — <em>это свойство кэшей, подразумевающее консистентность (целостность) значений, которые записываются в локальные кэши каждого из процессорных ядер.</em></p>

<p>Обеспечивается эта консистеность наличием у <strong>каждой</strong> ячейки кэша специальных флагов, от значений которых зависит то, как состояние хранимое в этой ячейке будет соотноситься с состояниями в ячейка кэша других ядер <strong>имеющих такой же адрес</strong>.</p>

<p>Например, когда состояние определенной ячейки меняется каким-либо образом, по внутренней сети процессорной системы происходит рассылка специльных сообщений (очень-очень быстро!).</p>

<p>На данный момент разработано много <em>протоколов когерентности</em>, которые отличаются алгоритмами работы и количеством состояний ячеек (флажков). Большая часть протоколов основаны на протоколе MESI (футболист тут не при чем!).</p>

<h4 id="протокол-когерентности-mesi">Протокол когерентности MESI</h4>

<p>Важно подчеркнуть то, что данные между ячейками оперативной памяти и ячейками процессорного кэша передаются блоками фиксированных размеров, которые называют <strong>линиями кэша</strong>.</p>

<p>В схеме работы протокола MESI каждая линия кэша может находиться в одном из четырех состояний (те самые флаги):</p>

<ol>
  <li>
    <p>Модифицированная линия (modified). Этим флагом одновременно может быть помечена только <em>одна</em> линия в <em>одном</em> локальном кэше. Очевидно из названия — флаг обозначает, что линия была как-то изменена, <strong>но до оперативной памяти изменения ещё не доехали</strong>. Ядро, к которому непосредственно относится эта линия может <em>без рассылки</em> уведомлений по внутреннй сети продолжать читать из ячейки и записывать в неё.</p>
  </li>
  <li>
    <p>Эксклюзивная линия (exclusive). Так же как и модифицированная линия, эксклюзивная может находиться одновременно только в одном локальном кэше. Отличие в том, что данные в этой линии <em>идентичны</em> данным в соответствующей ячейке оперативной памяти. Чтение и запись в этой линии происходит без уведомлений, но после изменения данных линия помечается как <em>модифицированная</em>.</p>
  </li>
  <li>
    <p>Разделямая линия (shared). Такая линия <em>может</em> содержаться одновременно в разных локальных кэшах, но запросы на изменение <strong>всегда</strong> отправляются в общую процессорную шину. Это приводит к тому, что все кэши других линий с таким же адресом отмечаются специальным флагом как <em>недействительные</em>.</p>
  </li>
  <li>
    <p>Недействительная линия (invalid). Попытка чтения из кэша недействительной линии всегда проваливается (cache miss). Chace miss приводит к тому, что “свежие” данные должны быть прочитаны из оперативной памяти. Этим флагом помечаются либо пустые линии, либо содержащие устаревшие значения.</p>
  </li>
</ol>

<hr />

<p>На практике работа процессорных кэшей, их внутреннее взаимодействиее и работа с оперативной памятью выглядит намного сложнее. Тему “железного” параллелизма мне захотелось затронуть, так как она является частью одного из вопросов в заключительном тесте по текущему курсу, и в качестве дополнительного акцента на разнице между concurrency и parallelism. Хочется верить что пост получился читаемым. Всех благ!</p>
:ET