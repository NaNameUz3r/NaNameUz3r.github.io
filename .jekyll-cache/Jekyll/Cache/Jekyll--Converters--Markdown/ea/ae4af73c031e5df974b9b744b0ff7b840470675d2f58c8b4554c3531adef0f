I"K<p>Как правильно думать о программе, что такое временная сложность и это самое “О большое”. Давайте разбираться.</p>

<!--more-->

<p>Недавно я сделал перевод важной статьи про <a href="/2022/01/15/programming_interview_algorithm.html">Алгоритм прохождения интервью по программированию</a>. Это сложная тема, особенно если вы никогда до этого не изучали структуры данных, ничего не слышали про временную сложность и не занимались оценкой эффективности или не эффективности программного кода.</p>

<h2 id="а-как-думать">А как думать?</h2>

<p>Начну с того что не зависимо от особенностей каждого человека, если мы физически и психически здоровы то “понимание” какой-то темы у нас складывается примерно одинково. Возьмем, например, телевизор. Человек растет и развивается, видит как его родители нажимают на кнопки, из телевизора показывают картинки, в его мозгу формируются некоторые связи приводящие к пониманию зависимости действий над объектом, и к каким результатам эти действия приведут. Но если человек никогда не интересовался как работает телевизор, из каких компонентов он состоит то он не сможет объяснить что это на самом деле такое, как работе, как это создать с нуля или починить. Это нормально. Инженер или работник завода на котором собирают телевизоры или его компоненты знают порядком больше, в виду обладания <em>большим опытом</em> в непосредственной области, соответствующим обучением, etc. Надо ли говорить как телевизор будет воспринимать пещерный человек, или коренной житель глубокой тропической деревни, где иногда до сих пор практикуют каннибализм?</p>

<p>С программированием всё обстоит так-же. Информационные технологии и программирование — дети науки и математики как науки, развивашихся последние столетия.
Это сложная тема, и нахрапом её не взять.</p>

<p>Когда мы программисты-самоучки, мы можем достаточно быстро выучить современный высокоуровневый и выразительный язык программирования, например Python, методы его стандартной библиотеки, и интуитивно начать собирать в голове образ того что мы хотим от программы, смутно представлять как она работает. Мы, ещё раз, <em>интуитивно</em> понимаем что такое переменная, что такое лист или массив. Ну есть там какие-то данные, мы их туда записали, можем сделать клац-клац <strong>sorted(my_very_spetial_list)</strong> и знаем чего ожидать. Несмотря на то, что таким же образом можно изучить ряд технологий, фреймворков и довольно успешно выполнять простые прикладные задачи, это представление неверно, контр-продуктивно и ничем хорошим не заканчивается, как только мы попробуем разработать сложную систему.</p>

<p>Поэтому единственный правильный способ думать о программе — это думать научно, и так же подходить к изучению программирования. Напрягаем извилины и идём дальше.</p>

<h2 id="алгоритм">Алгоритм</h2>

<p>Что такое программирование вообще? Ну, в первую очередь, скажете вы — это написание какой-то программы, которая что-то внутри делает и выдает интересующий нас результат, и будете правы, интуитивно-наивной точки зрения. Приемлемо. А что такое программа? Ни больше ни меньше, программа, в самом простом её представлении и правильном понимании — это <em>Алгоритм</em> достижения требуемого результата. Самый простой пример алгоритма, который я неоднократно встречал во всевозможных книжках начального уровня, это список ваших последовательных дел на завтра:</p>
<ul>
  <li>Проснуться</li>
  <li>Потянуться</li>
  <li>Надеть носки</li>
  <li>Поехать на работу (или дойти до компьютра и начать рабоать :) )</li>
  <li>Прерваться на обед</li>
</ul>

<p>При том каждый из пунктов тоже на самом деле является алгоритмом некоторых последовательных (а может и параллельных) действий, и в примере выше, я скоре описываю список алгоритмов.
Так и есть. В любой даже маленькой программе <em>n</em> число алгоритмов — вызовы разных функций, каждая из которых что-то вычисляет.
А если вычисляет — значит алгоритм. Кажется я вас начинаю запутывать, но думаю идея ясна.</p>

<p><em>Алгоритмом мы называем пошаговое руководство “компьютеру” что-то нам посчитать (вычислить). Вычисления могут быть очень простыми, а могут быть и крайне сложными. Правильный способ думать о программе не как о волшебном коде, а как о вполне строгом, <strong>математическом</strong> описании решения задачи.</em></p>

<p>Вычислительное железо, как и любой другой объект физического мира подчиняется физическим законам. Сила притяжения (на нашей планете) не даст вам прыгнуть кузнечиком в окно второго этажа. Вычислительные ресурсы (центральный процессор, оперативная память, etc) просто так не обработают по щелчку пальцев огромное количество данных — на это потребуется <strong>время</strong>.</p>

<p>К какому бы крутому суперкомпьютеру у вас вдруг не оказался доступ, его мощность играет <em>намного меньшее значение</em> для производительности программы, чем <em>правильно подобранный алгоритм</em>. Именно о правильном подборе алгоритма и идет речь в переведенной мною статье, со ссылки на которую начинается этот пост.</p>

<p>Итак, алгоритм — это вычисление или преобразование <em>входных</em> данных в интересующие нас <em>выходные</em> данные. Вы любите ждать? Маловероятно. Поэтому проще всего эффективность алгоритма понять как отношение времени, за которое мы получим результат. Чем быстрее — тем лучше.</p>

<h3 id="асимптотическая-сложность">Асимптотическая сложность</h3>

<p>Так что же влияет на скорость работы алгоритма? Как сказанно выше, в первую очередь это правильность самого алгоритма. А точнее — оправданность его использования в <em>конкретных обстоятельствах</em>. Под конкретным обстоятельствами я имею в виду контекст решаемой нами задачи. В контексте задачи скорость, с которой алгоритм будет вычислять решенит, зависит от <em>объема (количества) входных данных</em>.</p>

<p>Научно (математически) сложность алгоритмов оценивается с помощью так называемого <em>асимптотического анализа</em>. Итак, мы рассматриваем сложность алгоритма через анализ зависимости времени и вычислительных ресурсов, которые потребуются алгоритму в процессе его работы.</p>

<p>На практике всё это выражается как математическая функция, которая позволяет понять <em>насколько быстро</em> будет увеличиваться время работы алгоритма, в зависимости от увеличения обрабатываемого объема данных. Иными словами, под асимптотикой мы понимаем характер с которым изменяется функция в течении времени, когда её аргумент (количество обрабатываемых данных) стремится к какой-то точке. Только в асимптотическом анализе сложности <em>компьютерых алгоритмов</em> этой точкой является бесконечность. Мы оцениваем скорость работы алгоритма в сравнительном <em>количестве операций</em>.</p>

<p>Наиболее часто для оценки роста используются следующие <a href="https://en.wikipedia.org/wiki/Mathematical_notation">нотации</a>:</p>
<ul>
  <li>Ο (О-большое) – верхняя или “наихудшая” оценка сложности. Входные данные подаются самым “неудобным” для алгоритма способом;</li>
  <li>o (o-малое) – средняя оценка, данные подаются случайным образом;</li>
  <li>Ω (Омега) – нижняя оценка, идеальная ситуация для достижения максимальной скорости алгоритма;</li>
  <li>Θ (Тета) – нижняя и верхняя, точная оценка роста временной функции.</li>
</ul>

<p>Все эти нотации используются мощными человеческими умами в теоретической Computer Science, нам же полезнее всего будет рассмотреть фундаментальные меры сложности, использумые повсеместно, которые выражаются через O-большое.</p>

<p>Возьмем <em>n</em> за величину подаваемого объема данных. Тогда верхнюю оценку сложности алгоритма можно представить как <strong>O(f(n))</strong>. Почему нас интересует именно верхняя оценка? Потому что рассматривать <em>наихудший вариант развития событий</em> при котором алгоритму будет скормлено <em>большое количество данных</em>.</p>

<p>Самые простые для понимания обозначений сложности это <strong>O(1)</strong> и <strong>O(n)</strong>.</p>

<p><strong>O(1)</strong> выражает сложность, при которой для выполнения алгоритму потребуется <em>константа</em> времени. Например взятие элемента из массива по его индексу. Мы точно знаем где элемент находится и просто идём туда. <em>Такие операции не зависят от количества входных данных.</em></p>

<p><strong>O(n)</strong> линейно зависит от количества входных данных. В худшем случае такому линейному алгоритму придется проводить какие-то операции с каждым элементом входных данных. Это, например, обход массива, обох связанного списка или расчет его длинны. Чтобы понять лучше приведу утрированный пример. Представим что задача состоит в том, чтобы выпить n количество воды. Смоделируем ситуацию, в которой воду пьёт не человек, а джин-траглодит, у которого не желудок, а бездонная черная дыра (n стремится к бесконечности). Пить воду можно только последовательно, глоток за глотком, поэтому алгоритм выпивания воды будет иметь линейную сложность O(n) — чем больше воды мы дадим траглодиту на выпивание, тем дольше он будет её пить.</p>

<p>Классическим вариантом логарифимической сложности <strong>O(log n)</strong>, где за основание логарифма обычно берется 2, является бинарный (двоичный поиск). Чем больше входных данных — тем медленнее растет время выполнения алгоритма (тем меньше его сложность). Самый простой и затертый до дыр пример представления двоичного поиска — осуществление поиска номера телефона нужного вам человека по индексированному справочнику (Аббоненты упорядочены по алфовиту.) Вы можете искать человека по порядку, запись за записью, лист за листом. Согласитесь, очевидно что это займет много времени. Сложность такого алгритма можно выразить предыдущей формулой O(n). Представьте что вы ищите человека не в своей записной книжке, а в полном справочнике Москвы например. Намного быстрее, зная что справочник упорядочен открыть его примерно в том месте, где мы ожидаем найти аббонента. Например мы ищем Фёдора Корнельчука. Зная алфавит мы можем быть уверенны в том, что буква К находится в первой половине справочника, но мы совершенно не знаем сколько фамилий на А, Б, и так далее. Может оказаться что К находится во второй половине сравочника. Довичный поиск работает примерно так — мы открываем справочник (массив) посередине и смотрим куда попали, если нам нужно в левую половину, мы отбрасываем правую и открываем оставшуюся часть снова посередине. И так до тех пор пока не найдем Корнельчука.</p>

<p>Сложность алгоритмов сортировки часто выражается как <strong>O(n log n)</strong>. Время выполнения таких алгоритмов растет быстрее, чем O(n). Можно представить как комбинацию O(log n) и O(n).</p>

<p>Далее по интенсивности роста идут квадратичные зависимости, например <strong>O(n<sup>2</sup>)</strong>. Время выполнения таких алгоритмов растет очень быстро при увеличении объема входных данных (экспоненциально!). Например, два цикла, когда один вложен в другой, каждый из которых работает за O(n). Практически в любом случае это <strong>очень плохая мера</strong>, которую вообще не стоит определять в своих алгоритмах.</p>

<hr />

:ET